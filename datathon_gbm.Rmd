---
title: "Datathon"
author: "Eric Yi, ecyi"
date: "3/23/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

setwd("/Users/EricYi/Desktop/Code/datathon/")

library(ElemStatLearn)
library(caret)
library(caretEnsemble)
library(plyr)
library(plot3D)
library(gbm)

set.seed(0)

# The maximum number of trees to train with:
# 
n_trees = 400

# The max number of trees we want to grow out for spam:
#

n_spamtrees = 300

ggovindt_315_theme <- theme(
  panel.background = element_rect(fill = "white"),
  axis.title.x = element_text(family = "Garamond", size = 14, color = "black"),
  axis.title.y = element_text(family = "Garamond", size = 14, color = "black"),
  plot.title = element_text(family = "Garamond", size = 18, color = "black"),
  legend.text = element_text(family = "Garamond", size = 10, color = "black"),
  legend.position = "right",
  legend.title = element_text(family = "Garamond", size = 12, color = "black"),
  axis.ticks = element_line(color = "black"),
  axis.text = element_text(family = "Garamond", size = 10, color = "black")
)

green_df = read.csv("green_final.csv", header=T)
yellow_df = read.csv("yellow_final.csv", header=T)
```

```{r 10.2, cached=TRUE}
set.seed(0)

gen_eq_10_2_data = function(N=100,p=10){
    X = matrix( rnorm( N*p ), nrow=N, ncol=p )
    Y = matrix( -1, nrow=N, ncol=1 )
    threshold = qchisq(0.5,df=p)
    indx = rowSums( X^2 ) > threshold
    Y[indx] = +1
    data.frame(X,Y)
}
```

```{r}

```


```{r}
# p = 10 # the dimension of the feature vector (a "p" dimensional vector)
# N_train = 2000 # number samples to use in training
# N_test = 10000 # number of samples to use in testing 
# 
# # Extract the data we will to classify: 
# D_train = gen_eq_10_2_data(N=N_train,p=p)
# D_train[ D_train$Y==-1, p+1 ] = 0 # Map the response "-1" to the value of "0" (required format for the call to gbm): 
# 
# # Generate the formula used to fit our model with: 
# #
# terms = paste( colnames(D_train)[1:p], collapse="+" ) # dont consider the last column (the response variable)
# formula = formula( paste( colnames(D_train)[p+1], " ~ ", terms ) )
# 
# # Do training with the maximum number of trees: 
# #
# n_trees = 400
# if( T ){
#   print( "Running Adaboost ..." )
#   m = gbm( formula, data=D_train, distribution='adaboost', n.trees=n_trees, shrinkage=1, verbose=FALSE )
# }else{
#   print( "Running Bernoulli Boosting ..." )
#   m = gbm( formula, data=D_train, distribution='bernoulli', n.trees=n_trees, verbose=FALSE )
# }
# 
# # Lets plot the training error as a function of the number of trees:
# #
# # Note that when using the "bernoulli" output to compute the output of predict into a probability one needs to use:
# # plogis( predict( m, D_train[,1:p], n.trees=nti ) ) 
# #
# training_error = matrix( 0, nrow=n_trees, ncol=1 )
# for( nti in seq(1,n_trees) ){
#   Fhat = predict( m, D_train[,1:p], n.trees=nti )
#   pcc = mean( ( ( Fhat <= 0 ) & ( D_train[,p+1] == 0 ) ) | ( ( Fhat > 0 ) & ( D_train[,p+1] == 1 ) ) )
#   training_error[nti] = 1 - pcc
# }
# 
# # Lets plot the testing error as a function of the number of trees:
# #
# D_test = gen_eq_10_2_data(N=N_test,p=p)
# D_test[ D_test$Y==-1, p+1 ] = 0 # Map the response "-1" to the value of "0" (required format for the call to gbm): 
# 
# test_error = matrix( 0, nrow=n_trees, ncol=1 )
# for( nti in seq(1,n_trees) ){
#   Fhat = predict( m, D_test[,1:p], n.trees=nti )
#   pcc = mean( ( ( Fhat <= 0 ) & ( D_test[,p+1] == 0 ) ) | ( ( Fhat > 0 ) & ( D_test[,p+1] == 1 ) ) )
#   test_error[nti] = 1 - pcc 
# }
# 
# 
# #postscript("../../WriteUp/Graphics/Chapter10/dup_fig_10_2.eps", onefile=FALSE, horizontal=FALSE)
# 
# plot( seq(1,n_trees), training_error, type="l", main="Boosting Probability of Error", col="red", xlab="number of boosting iterations", ylab="classification error" )
# lines( seq(1,n_trees), test_error, type="l", col="green" )
# 
# legend( 275, 0.45, c("training error", "testing error"), col=c("red", "green"), lty=c(1,1) )
# 
# #dev.off()
```

```{r}
testing_error_vs_n_boosts = function(m,D_test){
  # 
  # Estimate the test set learning curve 
  # 
  
  n_trees = m$n.trees
  
  test_error = matrix( 0, nrow=n_trees, ncol=1 )
  for( nti in seq(1,n_trees) ){
    Fhat = predict( m, D_test[,1:p], n.trees=nti )
    pcc = mean( ( ( Fhat <= 0 ) & ( D_test[,p+1] == 0 ) ) | ( ( Fhat > 0 ) & ( D_test[,p+1] == 1 ) ) )
    test_error[nti] = 1 - pcc 
  }

  test_error 
}

p = 10 # the dimension of the feature vector (a "p" dimensional vector)
N_train = 2000 # number samples to use in training
N_test = 10000 # number of samples to use in testing 

# Extract the data we will to classify: 
D_train = gen_eq_10_2_data(N=N_train,p=p)
D_train[ D_train$Y==-1, p+1 ] = 0 # Map the response "-1" to the value of "0" (required format for the call to gbm): 

# Generate the formula used to fit our model with: 
#
terms = paste( colnames(D_train)[1:p], collapse="+" ) # dont consider the last column (the response variable)
formula = formula( paste( colnames(D_train)[p+1], " ~ ", terms ) )

# Extract the data we will test with: 
D_test = gen_eq_10_2_data(N=N_test,p=p)
D_test[ D_test$Y==-1, p+1 ] = 0 # Map the response "-1" to the value of "0" (required format for the call to gbm): 

# The maximum number of trees to train with:
# 
n_trees = 150
```

```{r}
print( "Training Adaboost ..." )
m = gbm( formula, data=D_train, distribution='adaboost', n.trees=n_trees, shrinkage=1, verbose=FALSE )
ab_testing_error = testing_error_vs_n_boosts(m,D_test)

print( "Running Binomial Deviance Boosting J=1 (stumps) ..." )
m = gbm( formula, data=D_train, distribution='bernoulli', interaction.depth=1, shrinkage=1, n.trees=n_trees, verbose=FALSE )
bd1_testing_error = testing_error_vs_n_boosts(m,D_test)

print( "Running Binomial Deviance Boosting J=6 ..." )
m = gbm( formula, data=D_train, distribution='bernoulli', interaction.depth=6, shrinkage=1, n.trees=n_trees, verbose=FALSE )
bd6_testing_error = testing_error_vs_n_boosts(m,D_test)

print( "Running Binomial Deviance Boosting J=20 ..." )
m = gbm( formula, data=D_train, distribution='bernoulli', interaction.depth=20, shrinkage=1, n.trees=n_trees, verbose=FALSE )
bd20_testing_error = testing_error_vs_n_boosts(m,D_test)
print("DONE")
```

```{r}
# Lets plot the testing error as a function of the number of trees for each of the potential models:
#
#postscript("../../WriteUp/Graphics/Chapter10/dup_fig_10_9.eps", onefile=FALSE, horizontal=FALSE)

plot( seq(1,n_trees), ab_testing_error, ylim=c(0,0.4), type="l", main="Boosting Probability of Error", col="gray", xlab="number of boosting iterations", ylab="classification error" )
lines( seq(1,n_trees), bd1_testing_error, type="l", col="black" )
lines( seq(1,n_trees), bd6_testing_error, type="l", col="green" )
lines( seq(1,n_trees), bd20_testing_error, type="l", col="red" )

legend( 100, 0.38, c("AdaBoost", "Stumps", "6 Node", "20 Node"), col=c("gray", "black", "green", "red"), lty=c(1,1) )

#dev.off()
```

```{r Hastie_spam_load}
load_spam_data <- function(trainingScale=TRUE,responseScale=TRUE){ 
  #
  # R code to load in the spam data set from the book ESLII
  #
  # Output:
  #
  # res: list of data frames XT
  #
  # Written by:
  # -- 
  # John L. Weatherwax                2009-04-21
  # 
  # email: wax@alum.mit.edu
  # 
  # Please send comments and especially bug reports to the
  # above email address.
  # 
  #-----

  X      = read.table("https://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/spam.data")
  tt     = read.table("https://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/spam.traintest")

  # separate into training/testing sets
  # (ESLII on epage 319 says that we have 3065 training instances)
  # 
  XTraining = subset( X, tt==0 )
  p = dim(XTraining)[2]-1
  
  XTesting  = subset( X, tt==1 ) # (ESLII on epage 319 says that we have 1536 testing instances)

  #
  # Sometime data is processed and stored in a certain order.  When doing cross validation
  # on such data sets we don't want to bias our results if we grab the first or the last samples.
  # Thus we randomize the order of the rows in the Training data frame to make sure that each
  # cross validation training/testing set is as random as possible.
  # 
  if( FALSE ){
    nSamples = dim(XTraining)[1] 
    inds = sample( 1:nSamples, nSamples )
    XTraining = XTraining[inds,]
  }

  #
  # In reality we have to estimate everything based on the training data only
  # Thus here we estimate the predictor statistics using the training set
  # and then scale the testing set by the same statistics
  # 
  if( trainingScale ){
    X = XTraining 
    if( responseScale ){
      meanV58 = mean(X$V58) 
      v58 = X$V58 - meanV58 
    }else{
      v58 = X$V58 
    }
    X$V58 = NULL
    X = scale(X, TRUE, TRUE)
    means = attr(X,"scaled:center")
    stds = attr(X,"scaled:scale")
    Xf = data.frame(X)
    Xf$V58 = v58
    XTraining = Xf

    # scale the testing predictors by the same amounts:
    # 
    DCVTest  = XTesting
    if( responseScale ){
      v58Test = DCVTest$V58 - meanV58
    }else{
      v58Test = DCVTest$V58 # in physical units (not mean adjusted)
    }
    DCVTest$V58 = NULL 
    DCVTest  = t( apply( DCVTest, 1, '-', means ) ) 
    DCVTest  = t( apply( DCVTest, 1, '/', stds ) ) 
    DCVTestb = cbind( DCVTest, v58Test ) # append back on the response
    DCVTestf = data.frame( DCVTestb ) # a data frame containing all scaled variables of interest
    names(DCVTestf)[p+1] = "V58" # fix the name of the response
    XTesting = DCVTestf
  }

  # Many algorithms wont do well if the data is presented all of one class and
  # then all of another class thus we permute our data frames :
  #
  XTraining = XTraining[sample(nrow(XTraining)),]
  XTesting  = XTesting[sample(nrow(XTesting)),]

  # Read in the list of s(pam)words (and delete garbage characters):
  # 
  spam_words = read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names",skip=33,sep=":",comment.char="|",stringsAsFactors=F)
  spam_words = spam_words[[1]]
  for( si in 1:length(spam_words) ){
    spam_words[si] = sub( "word_freq_", "", spam_words[si] )
    spam_words[si] = sub( "char_freq_", "", spam_words[si] )
  }

  return( list( XTraining, XTesting, spam_words ) ) 
}
```

```{r spam model}
PD         = load_spam_data(trainingScale=FALSE,responseScale=FALSE) # read in unscaled data
p          = dim(PD[[1]])[2]-1 # the last column is the response 1=>Spam; 0=>Ham
XTraining  = PD[[1]]; colnames(XTraining)[p+1] = "Y"
XTesting   = PD[[2]]; colnames(XTesting)[p+1] = "Y"
spam_words = PD[[3]]

# Generate the formula used to fit our model with: 
#
terms = paste( colnames(XTraining)[1:p], collapse="+" ) # dont consider the last column (the response variable)
formula = formula( paste( colnames(XTraining)[p+1], " ~ ", terms ) )

n_trees = 100
K = 3 # larger values of K can give different performance
m = gbm( formula, data=XTraining, distribution='bernoulli', n.trees=n_trees, shrinkage=0.005, interaction.depth=K, verbose=TRUE, cv.folds=5 )

# This works but I think the default method of gbm.perf(m,method="cv")
# provides a better esimate of performance as a function of boosting iteration : 
# 
if( FALSE ){ 
    # Simple sanity check that our algorithm is working correctly (plot training as a function of number of boosts): 
    #
    training_error = matrix( 0, nrow=n_trees, ncol=1 )
    for( nti in seq(1,n_trees) ){
      Fhat = predict( m, XTraining[,1:p], n.trees=nti )
      pcc = mean( ( ( Fhat <= 0 ) & ( XTraining[,p+1] == 0 ) ) | ( ( Fhat > 0 ) & ( XTraining[,p+1] == 1 ) ) )
      training_error[nti] = 1 - pcc
    }

    # Lets plot the testing error as a function of the number of trees:
    #
    test_error = matrix( 0, nrow=n_trees, ncol=1 )
    for( nti in seq(1,n_trees) ){
      Fhat = predict( m, XTesting[,1:p], n.trees=nti )
      pcc = mean( ( ( Fhat <= 0 ) & ( XTesting[,p+1] == 0 ) ) | ( ( Fhat > 0 ) & ( XTesting[,p+1] == 1 ) ) )
      test_error[nti] = 1 - pcc 
    }

    plot( seq(1,n_trees), training_error, type="l", main="Boosting Probability of Error", col="red", xlab="number of boosting iterations", ylab="classification error" )
    lines( seq(1,n_trees), test_error, type="l", col="green" )

    legend( 700, 0.4, c("training error", "testing error"), col=c("red", "green"), lty=c(1,1) )
}

# Look at the learning curves (estimated via cross validation) and determine the best number of boosting trees to use:
# 
#postscript("../../WriteUp/Graphics/Chapter10/dup_spam_learning_curves_K_3.eps", onefile=FALSE, horizontal=FALSE)
best.iter = gbm.perf(m,method="cv")
#dev.off()

# Plot the relative importance of each word used in the classification:
#
if( FALSE ){
  rel_importance = summary(m,n.trees=best.iter)
  
  # Get the relative importance of each word (normalized to 100 for the most important word): 
  max_value = rel_importance[[2]][1]
  rel_importance_norm_values = ( rel_importance[[2]]/max_value ) * 100.

  # Get the list of words ordered by the most important: 
  word_indx = as.double( sub( "V", "", rel_importance[[1]] ) )
  most_important_words = spam_words[ word_indx ] 

  #postscript("../../WriteUp/Graphics/Chapter10/dup_spam_rel_importance.eps", onefile=FALSE, horizontal=FALSE)
  barplot( rel_importance_norm_values, horiz=TRUE, names.arg=most_important_words )
  #dev.off()
}

# Lets evaluate the testing error given as a function of the optimal number of trees: 
#
Fhat = predict( m, XTesting[,1:p], n.trees=best.iter )
pcc = mean( ( ( Fhat <= 0 ) & ( XTesting[,p+1] == 0 ) ) | ( ( Fhat > 0 ) & ( XTesting[,p+1] == 1 ) ) )
print( 1 - pcc )
```

```{r test_shrinkage}
# Trying to use LASSO for post-processing
m = gbm( formula, data=XTraining, distribution='bernoulli', n.trees=10, shrinkage=0.05, interaction.depth=5, verbose=TRUE, cv.folds=5 )
shrink.gbm(m, n.trees=10)
```

```{r spam}

n_trees = 300

# Method 1: Use Their Stuff

# Method 2: Resample our own sets
# XAdded = rbind(XTesting, XTraining)
# 
# X_sampled = sample(nrow(XAdded), 4601, replace=FALSE)
# 
# XTraining = XAdded[X_sampled[1:3065],]
# 
# XTesting = XAdded[X_sampled[3066:4601],]
#

terms = paste( colnames(XTraining)[-58], collapse="+" )
formula = formula( paste( colnames(XTraining)[58], " ~ ", terms ) )

testing_error_vs_n_boosts2 = function(m,D_test){
  #
  # Estimate the test set learning curve
  #

  n_trees = m$n.trees

  test_error = matrix( 0, nrow=n_trees, ncol=1 )
  for( nti in seq(1,n_trees) ){
    Fhat = predict( m, D_test[,-58], n.trees=nti )
    pcc = mean( ( ( Fhat <= 0 ) & ( D_test[,"Y"] == 0 ) ) | ( ( Fhat > 0 ) & ( D_test[,"Y"] == 1 ) ) )
    test_error[nti] = 1 - pcc
  }

  test_error
}

print( "Training Adaboost ..." )
m = gbm( formula, data=XTraining,
         distribution='adaboost', n.trees=n_trees, shrinkage=.1,
         train.fraction=0.9, verbose=FALSE )
ab_testing_error = testing_error_vs_n_boosts2(m,XTesting)
ab_internal = m$valid.error

print( "Running Binomial Deviance Boosting J=1 (stumps) ..." )
m = gbm( formula, data=XTraining,
         distribution='bernoulli', interaction.depth=1, shrinkage=.1, n.trees=n_trees,
         train.fraction=0.9, verbose=FALSE )
bd1_testing_error = testing_error_vs_n_boosts2(m,XTesting)
bd1_internal = m$valid.error

print( "Running Binomial Deviance Boosting J=6 ..." )
m = gbm( formula, data=XTraining,
         distribution='bernoulli', interaction.depth=6, shrinkage=.1, n.trees=n_trees,
         train.fraction=0.9, verbose=FALSE )
bd6_testing_error = testing_error_vs_n_boosts2(m,XTesting)
bd6_internal = m$valid.error

print( "Running Binomial Deviance Boosting J=20 ..." )
m = gbm( formula, data=XTraining,
         distribution='bernoulli', interaction.depth=20, shrinkage=.1, n.trees=n_trees,
         train.fraction=0.9, verbose=FALSE )
bd20_testing_error = testing_error_vs_n_boosts2(m,XTesting)
bd20_internal = m$valid.error
```

```{r}
plot( seq(1,n_trees), ab_testing_error, ylim=c(0,0.4), type="l", main="Boosting Probability of Error", col="gray", xlab="number of boosting iterations", ylab="classification error" )
lines( seq(1,n_trees), bd1_testing_error, type="l", col="black" )
lines( seq(1,n_trees), bd6_testing_error, type="l", col="green" )
lines( seq(1,n_trees), bd20_testing_error, type="l", col="red" )

legend( 100, 0.38, c("AdaBoost", "Stumps", "6 Node", "20 Node"), col=c("gray", "black", "green", "red"), lty=c(1,1) )
```

```{r}
plot( seq(1,n_trees), ab_internal, ylim=c(0,0.4), type="l", main="Boosting Validation Deviance", col="gray", xlab="number of boosting iterations", ylab="classification error" )
lines( seq(1,n_trees), bd1_internal, type="l", col="black" )
lines( seq(1,n_trees), bd6_internal, type="l", col="green" )
lines( seq(1,n_trees), bd20_internal, type="l", col="red" )

legend( 200, 0.38, c("AdaBoost", "Stumps", "6 Node", "20 Node"), col=c("gray", "black", "green", "red"), lty=c(1,1) )
```

```{r}
#n_trees = 101
#errors = c()
#
#for (i in seq(1:49)){
#    m = gbm( formula, data=spam_train, distribution='bernoulli', interaction.depth=i, shrinkage=.1, n.trees=n_trees, bag.fraction=0.5, train.fraction=0.9, verbose=TRUE )
#    errors <- c(errors, testing_error_vs_n_boosts(m,spam_test)[100])
#    
#}
```

```{r}
# plot(seq(1:49), errors, main="Errors vs. Bottom Node Complexity for SGB")
```

```{r}
errors = c()
internal_errors = c()
for (i in seq(1, 49, 5)){
    print(cat("working..."))
    m = gbm( formula, data=XTraining,
             distribution='bernoulli', interaction.depth=i, shrinkage=0.05,
             n.trees=n_spamtrees, bag.fraction=0.75, train.fraction=0.9,
             verbose=FALSE )
    # m_shrink = shrink.gbm(m, n.trees=n_trees, lambda=rep(1, 57))
    errors <- c(errors, testing_error_vs_n_boosts2(m,XTesting))
    internal_errors <- m$valid.error
    }
```

```{r}
persp(x=seq(1:300),
      y=seq(1, 49, 5),
      z=matrix(errors, nrow=300, ncol=10, byrow=FALSE),
      xlab="Number of Iterations", ylab="Leaf Count", zlab="Error",
      main="Validation Error vs. SGBM Complexity and Rounds",
      phi=0, theta=40, border=NA, col=rainbow(3, alpha=0.6), ticktype="detailed",
      lphi=5, ltheta=35, shade=0.7)
```

```{r}
summary(m, n.trees=best.iter)
```

```{r}
partial <- plot(m, i.var=c(1,2), return.grid=TRUE)

colnames(partial)

mat <- reshape2::acast(data = partial, formula = V2 ~ V1, value.var = 'y') # matrix of values for 3D plot
```

```{r}
persp(x = as.numeric(colnames(mat)), y = as.numeric(rownames(mat)), z=mat,
      zlab = 'partial dependence', xlab = 'V1', ylab = 'V2',
      phi = 20, theta = 45, border=NA, col=rainbow(3, alpha=0.6))
```

```{r}
n_trees = 300
influence_vec = c()
feature = 1 # feature whose relative influence we want to plot vs. tree size
for (i in seq(1:n_trees)){
    var_inf_vec = relative.influence(m, n.trees=i)
    influence_vec <- c(influence_vec, var_inf_vec[feature])
}
```

```{r}
plot(seq(1:300), influence_vec, main="Relative Influence vs. Tree Size",
     xlab="Tree Size", ylab="Relative Influence")
```